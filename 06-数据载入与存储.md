```python
# import numpy as np
# import pandas as pd
# np.random.seed(12345)  
# import matplotlib.pyplot as plt
# plt.rc('figure',figsize=(10,6))
# np.set_printoptions(precision=4,suppress=True)
```


```python
# 文本格式的数据读写，pandas可以读取各种格式的数据
# pd.read_
# read_table 允许自己指定分隔符 sep=''
# 有的文件没有头行，设置header=None会默认给个数字当表头
# 用names=[]自己设置表头
# 指定某列为索引，可以用index_col=''
# 使用正则表达式做分隔符，有时一张表的分隔符不固定。如：文档是数量不确定的空格分隔，指定分隔符为正则表达式\s+
```


```python
# 处理缺失值
# 通常pandas中用NULL或NA来标记

# 判断是否为缺失值
# pd.isnull(result)

# na_values 可以传入一个类表或一组字符串来指定缺失值
# result=pd.read_csv('examples/ex5.csv',na_values=[NULL])

# 可以指定不同位置的内容用缺失值表示
# sentinels={'message':['foo','NA'],'something':['two']}
# pd.read_csv('examples/ex5.csv',na_values=sentinels)
```


```python
# 6.1.1 分块读入文本文件(Reading Text Files in Pieces)
# 对 pandas 的显示进行调整，只显示10列
# pd.options.display.max_rows=10
# result=pd.read_csv('example/ex6.csv')


# 指定行数，只读出前五行
# pd.read_csv('examples/ex6.csv',nrows=5)


# chunker就是分块的意思
# chunksize=1000，指定每一块的行数
# chunker=pd.read_csv('examples/ex6.csv',chunksize=1000)
# chunker
# 如：0-999是第一块，1000-1999是第二块儿，返回值为TextParser对象，允许遍历
# chunker=pd.read_csv('examples/ex6.csv',chunksize=1000)
# for piece in chunker:
#     print(piece)
# 1000行读取一次


# 统计某列值的数量
# chunker=pd.read_csv('examples/ex6.csv',chunksize=1000)
# tot=pd.Series([])
# for piece in chunker:
#     print(piece['key'].value_counts())


# 统计'key'这列每个元素出现的频率
# chunker=pd.read_csv('examples/ex6.csv',chunksize=1000)
# tot=pd.Series([])
# for piece in chunker:
#     tot=tot.add(piece['key'].value_counts(),fill_value=0)
# tot=tot.sort_values(ascending=False)
```


```python
# 6.1.2 将数据写入文本格式(Writing Data to Text Format)
# 读出文本，读出的变量存在data里
# data=pd.read_csv('examples/ex5.csv')
# data

# 写入文档
# data.to_csv('examples/out.csv')

# sys.stdout输出到屏幕，但是使用|分割
# import sys
# data.to_csv(sys.stdout,sep='|')

# 缺失值使用NULL填充
# data.to_csv(sys.stdout,na_rep='NULL')


# 写入文档，去掉index，去掉header
# data.to_csv(sys.stdout,index=False,header=False)

# 写入文档，列名为a，b，c
# data.to_csv(sys.stdout,index=False,columns=['a','b','c'])


# 生成时间数据，从2000年1月1日开始，期限为7天，生成的数据类型是datatime
# datas=pd.data_range('1/1/2000',periods=7)
# datas
# ts=pd.Series(np.arange(7),index=datas)
# ts


# 将文档写入文件
# ts.to_csv('examples/tseries.csv')
```


```python
# 6.1.3 处理分隔格式 Working with Delimited Formats
# 有些情况read_csv解决不了，例如：带双引号的文档，首先导入csv包，读出csv文件
# import csv
# f=open('examples/ex7.csv')
# reader=csv.reader(f)
# 读出来的reader是一个可迭代的对象，使用for循环读出来
# for line in reader:
#     print(line)
#     print(type(line))


# 将文件读取为行的列表
# with open('examples/ex7.csv') as f:
#     lines=list(csv.reader(f))
# header,values=lines[0],lines[1:]
# 再拆成字典格式
# data_dict={h:v for h,v in zip(header,zip(*values))}
# data_dict
```


```python
# 6.1.4 JSON Data
# obj="""    # 多行字符串
# {"name":"Wes",
#     "places_lived":["United States","Spain","Germany"],
#     "pet":"null",
#     "siblings":[{"name":"Scott","age":30,"pets":["Zeus","Zuko"]},
#                {"name":"Katie","age":38,"pets":["Sixes","Stache","Cisco"]}]
# }
# """
# 将json字符串转换为python形式
# import json
# import pandas as pd
# result=json.loads(obj)
# result
# # {'name': 'Wes',
# #  'places_lived': ['United States', 'Spain', 'Germany'],
# #  'pet': 'null',
# #  'siblings': [{'name': 'Scott', 'age': 30, 'pets': ['Zeus', 'Zuko']},
# #   {'name': 'Katie', 'age': 38, 'pets': ['Sixes', 'Stache', 'Cisco']}]}


# python形式转换为json
# asjson=json.dumps(result)
# asjson
# # '{"name": "Wes", 
# # "places_lived": ["United States", "Spain", "Germany"], 
# # "pet": "null", 
# # "siblings": [{"name": "Scott", "age": 30, 
# #               "pets": ["Zeus", "Zuko"]}, 
# #              {"name": "Katie", 
# #               "age": 38, 
# #               "pets": ["Sixes", "Stache", "Cisco"]}]}'


# 可以自由转换数据结构，如result变量里面的'sibling'对应的字典，字典里面取出name和age作为列
# siblings=pd.DataFrame(result['siblings'],columns=['name','age'])
# siblings
# # 	name	age
# # 0	Scott	30
# # 1	Katie	38


# pd.read_json可以直接报json数据集按照指定的次序转换为Series或者DataFrame
# data=pd.read_json('examples/example.json')
# data
# # 再转换为json
# print(data.to_json())
# # 保存
# print(data.to_json(orient='records'))
```


```python
# 6.1.5 网络抓取 XML and HTML:Web Scraping
# html和xml格式文件的读取
# pd.read_html有很多选项，但默认的情况下会搜索并尝试解析所有包含在标签中的表格型数据，返回的结果是DataFrame对象的列表
# tables=pd.read_html('examples/fdic_failed_bank_list.html')
# len(tables)
# failures=tables[0]
# failures.head()
# 可以进行一些简单的清理工作，如计算每年银行倒闭的数量
# close_timestamps=pd.to_datetime(failures['Closing Date'])
# colse_timestamps
# colse_timestamps.dt.year.value_counts()
```


```python
# 6.1.5.1 解析 XML使用 xml.objectify
# XML是一种常见的结构化数据格式，使用元数据支持分层，嵌套数据
# from lxml import objectify
# path='datasets/mta_perf/Performance_MNR.xml'
# parsed=objectify.parse(open(path))
# root=parsed.getroot()  # 获得对XML文件的根节点的引用
# root # 返回一个迭代器，可以产生每一个XML元素
# data=[]
# skip_fields=['PARENT_SEQ','INDICATOR_SEQ','DESIRED_CHANGE','DECIMAL_PLACES']
# # 这几个标签下的值不要
# for elt in root.INDICATOR:
#     # 一组数据
#     el_data={}
#     # 存储该数据的字典
#     for chilr in elt.getchildren():
#         # 取出一列数据
#         if child.tag in skip_fields:
#             # 如果这列的标签在上面定义的列表里
#             continue
#             # 舍弃，继续
#         el_data[child.tag]=child.pyval
#         # 如果不在，标签作为key值，这列的pyval作为value值加入字典
#     data.append(el_data)
#     # 该组数据加入总体数据列表
# len(data)
```


```python
# 6.2 二进制格式 Binary Data Formats
# frame=pd.read_csv('examples/ex1.csv')
# frame
# # 保存为二进制格式 pickle只能作为短期的存储格式，难以保证长期有效
# frame.to_pickle('examples/frame_pickle')
# pd.read_pickle('examples/frame_pickle')
```


```python
# 6.2.2 Reading Microsoft Excel Files
# xlsx=pd.ExcelFile('examples/ex1.xlsx')
# pd.read_excel(xlsx,'Sheet1')
# frame=pd.read_excel('examples/ex1.xlsx','Sheet1')
# frame
# writer=pd.ExcelWriter('examples/ex2.xlsx')
# frame.to_excel(writer,'Sheet1')
# writer.save()
# frame.to_excel('examples/ex2.xlsx')
```


```python
# 6.3 Interacting with Web APIs
# 获取github上最新的30条关于pandas的问题，发送一个HTTP GET请求
# import requests
# url='https://api.github.com/repos/pandas-dev/pandas/issues'
# resp=requests.get(url)
# resp
# # response对象的json方法将返回一个包含解析为本地python对象的JSON字典
# data=resp.json
# # data[0]['title']
# # data中的每个元素都是一个包含Github问题页面上所有数据的字典
# # 可以将data直接传给DataFrame，并提取感兴趣的字段
# issues=pd.DataFrame(data,columns=['number','title','labels','state'])
# issues
```


```python
# 6.4 与数据库交互 Interacting with Databases
# 通常企业中的数据不是存储在文本或 Excel文件中的，而是基于SQL的关系型数据库
# 执行顺序： 数据库连接，执行sql语句，提交，关闭连接
# 使用sqlite3创建数据库的连接
# import sqlite3
# import pandas as pd
# # 数据库连接，sql命令
# # 在python中用三引号创建一个多行文本变量，该变量为sql命令，用于创建一个名字叫test的表格，里面有4列数据
# query="""
# CREATE TABLE test
# (a VARCHAR(20),b VARCHAR(20),
# c REAL, d INTEGER
# )
# """
# # a和b字段是字符串类型，最长20，c是十进制8位存储，d是整数类型
# # 打开或创建数据库文件 当指定的数据库文件不存在的时候，连接对象会自动创建数据库文件，如果数据库文件已存在，直接打开该数据库文件
# con=sqlite3.connect('mydata_5.sqlite')   # .sqlite表面文件类型是文件型数据库
# con.execute(query)
# # 提交命令 对数据库产生修改的语句，要commit才会生效，如果是查询，就只要execute提交语句
# con.commit()
# # 执行多条命令并提交
# # 将数据一行一行插进去
# data=[('Atlanta','Georgia',1.25,6),
#       ('Tallahassee','Florida',2.6,3),
#       ('Sacramento','California',1.7,5)]
# stmt='INSERT INTO test VALUES(?,?,?,?)'
# con.executemany(stmt,data)
# con.commit()
# # 读出所有数据  select*from test  *是通配符，类似正则表达式，在test表格里读出所有数据
# cursor=con.execute('select*from test')
# cursor
# # 返回一个类似指针的东西，指向数据表中某一行，是一个可迭代对象，用fetchall()函数读出   <sqlite3.Cursor at 0x1bc401a0a40>
# rows=cursor.fetchall()
# # rows
# # # [('Atlanta', 'Georgia', 1.25, 6),
# # #  ('Tallahassee', 'Florida', 2.6, 3),
# # #  ('Sacramento', 'California', 1.7, 5)]

# # 读出表格内容，但没有读出列名，列名在cursor的description属性中 将他读出
# for x in cursor.description:
#     print(x)
# # # ('a', None, None, None, None, None, None)
# # # ('b', None, None, None, None, None, None)
# # # ('c', None, None, None, None, None, None)
# # # ('d', None, None, None, None, None, None)

# # 将列名作为DataFrame的columns，row作为每行的值，创建DataFrame
# cursor.description
# pd.DataFrame(rows,columns=[x[0] for x in cursor.description])
# con.close
# # 删除命令，不删除下次使用会报错
# !del mydata_5.sqlite
```
